{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SYDE 556/750: Simulating Neurobiological Systems\n",
      "\n",
      "Terry Stewart\n",
      "\n",
      "## Symbols and Symbol-like representation in neurons\n",
      "\n",
      "- We've seen how to represent vectors in neurons\n",
      "    - And how to compute functions on those vectors\n",
      "    - And dynamical systems\n",
      "    - And how to build up complex controllable models with the cortex-basal ganglia-thalamus loop\n",
      "- But how can we do anything like human language?\n",
      "    - How could we represent the fact that \"the number after 8 is 9\"\n",
      "    - Or \"dogs chase cats\"\n",
      "    - Or \"Anne knows that Bill thinks that Charlie likes Dave\"\n",
      "- Does the NEF help us at all with this?\n",
      "    - Or is this just too hard a problem yet?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Traditional Cognitive Science\n",
      "\n",
      "- Lots of theories that work with structured information like this\n",
      "- Pretty much all of them use some representation framework like this:\n",
      "    - `after(eight, nine)`\n",
      "    - `chase(dogs, cats)`\n",
      "    - `knows(Anne, thinks(Bill, likes(Charlie, Dave)))`\n",
      "- Or perhaps\n",
      "    - `[number:eight next:nine]`\n",
      "    - `[subject:dogs action:chase object:cats]`\n",
      "    - `[subject:Anne action:knows object:[subject:Bill action:thinks object:[subject:Charlie action:likes object:Dave]]]`\n",
      "- Cognitive models manipulate these sorts of representations\n",
      "    - mental arithmetic\n",
      "    - driving a car\n",
      "    - using a GUI\n",
      "    - parsing language\n",
      "    - etc etc\n",
      "- Seems to match well to behavioural data, so something like this should be right\n",
      "- So how can we do this in neurons?\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- This is a hard problem\n",
      "- Jackendoff (2002) posed this as a major problem\n",
      "- Four linguistic challenges for cognitive neuroscience\n",
      "    - The Binding Problem\n",
      "    - The Problem of 2\n",
      "    - The Problem of Variables\n",
      "    - Working Memory vs Long-term memory"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- The Binding Problem\n",
      "    - Suppose you see a red square and a blue circle\n",
      "    - How do you keep these ideas separate?  How does \"red\" get bound with \"square\", and how is it kept separate from \"blue\" and \"circle\"?\n",
      "- The Problem of 2\n",
      "    - \"The little star's beside the big star\"\n",
      "    - How do we keep those two uses of \"star\" separate?\n",
      "- The Problem of Variables\n",
      "    - Words seem to have types (nouns, verbs, adjectives, etc, etc)\n",
      "    - How do you make use of that?  \n",
      "    - E.g. \"blue *NOUN*\" is fine, but \"blue *VERB*\" is not (or is very different)\n",
      "- Working memory vs Long-term memory\n",
      "    - We can both use sentences (working memory) and store them indefinitely (long-term memory)\n",
      "    - How do these transfer back and forth?\n",
      "    - What are they in the brain?  This seems to require moving from storing something in neural activity to storing it in connection weights\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Possible solutions\n",
      "\n",
      "- Oscilations\n",
      "    - \"red square and blue circle\"\n",
      "    - Different patterns of activity for RED, SQUARE, BLUE, and CIRCLE\n",
      "    - Have the patterns for RED and SQUARE happen, then BLUE and CIRCLE, then back to RED and SQUARE\n",
      "    - More complex structures possible too:\n",
      "    - E.g. the LISA architecture\n",
      "    \n",
      "<img src=\"files/lecture8/lisa.png\">\n",
      "\n",
      "- Problems\n",
      "    - What controls this oscillation?\n",
      "    - How is it controlled?\n",
      "    - How do we deal with the exponentional explosion of nodes needed?\n",
      "    \n",
      "- Implementing Symbol Systems in Neurons\n",
      "    - Build a general-purpose symbol-binding system\n",
      "    - Lots of temporary pools of neurons\n",
      "    - Ways to temporarily associate them with particular concepts\n",
      "    - Ways to temporarily associate pools together\n",
      "    - Neural Blackboard Architecture\n",
      "    \n",
      "<img src=\"files/lecture8/nba.png\">\n",
      "\n",
      "- Problems\n",
      "    - Very particular structure (doesn't seem to match biology)\n",
      "    - Uses a very large number of neurons (~500 million) to be flexible enough for simple sentences\n",
      "    - And that's just to represent the sentence, never mind controlling and manipulating it\n",
      "- Deeper problem\n",
      "    - If we're just implementing something like the original symbol system in neurons, then we're reducing the neural aspects to *mere implementational details*\n",
      "    - That is, if we had a perfectly good way to implement symbol structures in neurons, then we could take existing cognitive theories based on symbol structures and implement them in neurons\n",
      "    - But so what?  That'd just be conceeding the point that the neurons don't matter -- you don't need them to develop the theory.  \n",
      "        - They're useful for testing some aspects, like what firing patterns should be\n",
      "        - But it's more for completeness sake than for understanding\n",
      "        - No more interesting than the question of \"well, how does that neuron get implemented in terms of chemistry\"  or atoms.  Or quarks.\n",
      "    - This is why the majority of cognitive scientists don't worry about neurons"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Vector Symbolic Architectures\n",
      "\n",
      "- There is an alternate approach\n",
      "- Something that's similar to the symbolic approach, but much more tied to biology\n",
      "    - Most of the same capabilities as the classic symbol systems\n",
      "    - But not all\n",
      "- Based on vectors and functions on those vectors\n",
      "    - There is a vector for each concept\n",
      "    - Build up structures by doing math on those vectors\n",
      "    \n",
      "- Example\n",
      "    - blue square and red circle\n",
      "    - can't just do BLUE+SQUARE+RED+CIRCLE\n",
      "        - why?\n",
      "    - need some other operation as well\n",
      "    - requirements\n",
      "        - input 2 vectors, get a new vector as output\n",
      "        - reversible (given the output and one of the input vectors, generate the other input vector)\n",
      "        - output vector is highly dissimilar to either input vector\n",
      "            - unlike addition, where the output is highly similar\n",
      "    - Any ideas for such a function?\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Lots of different options\n",
      "    - for binary vectors, XOR works pretty good\n",
      "    - for continuous vectors we use circular convolution\n",
      "- Why?\n",
      "    - Extensively studied (Plate, 1997: Holographic Reduced Representations)\n",
      "    - Easy to approximately invert (circular correlation)\n",
      "- `BLUE` $\\circledast$ `SQUARE + RED` $\\circledast$ `CIRCLE`\n",
      "\n",
      "<img src=\"files/lecture8/bind.png\">\n",
      "\n",
      "<img src=\"files/lecture8/unbind.png\">\n",
      "\n",
      "- Lots of nice properties\n",
      "    - Can store complex structures\n",
      "         - `[number:eight next:nine]`\n",
      "         - `NUMBER` $\\circledast$ `EIGHT + NEXT` $\\circledast$ `NINE`\n",
      "         - `[subject:Anne action:knows object:[subject:Bill action:thinks object:[subject:Charlie action:likes object:Dave]]]`  \n",
      "         - `SUBJ` $\\circledast$ `ANNE + ACT` $\\circledast$ `KNOWS + OBJ` $\\circledast$ `(SUBJ` $\\circledast$ `BILL + ACT` $\\circledast$ `THINKS + OBJ` $\\circledast$ `(SUBJ` $\\circledast$ `CHARLIE + ACT` $\\circledast$ `LIKES + OBJ` $\\circledast$ `DAVE))`\n",
      "    - But gracefully degrades!\n",
      "        - as representation gets more complex, the accuracy of breaking it apart decreases\n",
      "    - Keeps similarity information\n",
      "        - if `RED` is similar to `PINK` then `RED` $\\circledast$ `CIRCLE` is similar to `PINK` $\\circledast$ `CIRCLE`\n",
      "        \n",
      "- But rather complicated\n",
      "    - Seems like a weird operation for neurons to do"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Circular convolution in the NEF\n",
      "\n",
      "- Or is it?\n",
      "- Circular convolution is a whole bunch ($D^2$) of multiplies\n",
      "- But it can also be written as a fourier transform, an elementwise multiply, and another fourier transform\n",
      "- The discrete fourier transform is just a linear operation\n",
      "- So that's just $D$ pairwise multiplies\n",
      "- In fact, circular convolution turns out to be *exactly* what the NEF shows neurons are good at"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Jackendoff's Challenges\n",
      "\n",
      "- As pointed out in [Vector Symbolic Architectures Answer Jackendoff's Challenges for Cognitive Neuroscience](http://arxiv.org/ftp/cs/papers/0412/0412059.pdf)\n",
      "- The Binding Problem\n",
      "    - `RED` $\\circledast$ `CIRCLE + BLUE` $\\circledast$ `TRIANGLE`\n",
      "- The Problem of 2\n",
      "    - \"The little star's beside the big star\"\n",
      "    - `OBJ1` $\\circledast$ `(TYPE` $\\circledast$ `STAR + SIZE` $\\circledast$ `LITTLE) + OBJ2` $\\circledast$ `(TYPE` $\\circledast$ `STAR + SIZE` $\\circledast$ `BIG) + BESIDE` $\\circledast$ `OBJ1` $\\circledast$ `OBJ2`\n",
      "- The Problem of Variables\n",
      "    - `S` = `RED` $\\circledast$ `NOUN`\n",
      "    - `VAR` = `BALL` $\\circledast$ `NOUN'`\n",
      "    - `S` $\\circledast$ `VAR` = `RED` $\\circledast$ `BALL`\n",
      "- Working Memory vs Long-term memory\n",
      "    - vectors are what we work with (activity of neurons)\n",
      "    - functions are long-term memory (connection weights)\n",
      "    - functions return (and modify) vectors\n",
      "    \n",
      "    \n",
      "    \n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Symbol-like manipulation\n",
      "\n",
      "- Can do a lot of standard symbol stuff\n",
      "- Have to explicitly bind and unbind to manipulate the data\n",
      "- Less accuracy for more complex structures\n",
      "- But we can also do more with these representations\n",
      "\n",
      "<iframe width=\"640\" height=\"390\" src=\"//www.youtube.com/embed/Q_LRvnwnYp8\" frameborder=\"0\" allowfullscreen></iframe>\n",
      "\n",
      "- `S1 = ONE` $\\circledast$ `P1`\n",
      "- `S2 = ONE` $\\circledast$ `P1 + ONE` $\\circledast$ `P2`\n",
      "- `S3 = ONE` $\\circledast$ `P1 + ONE` $\\circledast$ `P2 + ONE` $\\circledast$ `P3`\n",
      "- `S4 = FOUR` $\\circledast$ `P1`\n",
      "- `S5 = FOUR` $\\circledast$ `P1 + FOUR` $\\circledast$ `P2`\n",
      "- `S6 = FOUR` $\\circledast$ `P1 + FOUR` $\\circledast$ `P2 + FOUR` $\\circledast$ `P3`\n",
      "- `S7 = FIVE` $\\circledast$ `P1`\n",
      "- `S8 = FIVE` $\\circledast$ `P1 + FIVE` $\\circledast$ `P2`\n",
      "\n",
      "- what is `S9`?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Let's figure out what the transformation is\n",
      "- `T1 = S2` $\\circledast$ `S1'`\n",
      "- `T2 = S3` $\\circledast$ `S2'`\n",
      "- `T3 = S5` $\\circledast$ `S4'`\n",
      "- `T4 = S6` $\\circledast$ `S5'`\n",
      "- `T5 = S8` $\\circledast$ `S7'`\n",
      "\n",
      "- `T = (T1 + T2 + T3 + T4 + T5)/5`\n",
      "- `S9 = S8` $\\circledast$ `T`\n",
      "\n",
      "- `S9 = FIVE` $\\circledast$ `P1 + FIVE` $\\circledast$ `P2 + FIVE` $\\circledast$ `P3`\n",
      "\n",
      "- This becomes a novel way of manipulating structured information\n",
      "    - Exploiting the fact that it is a vector underneath\n",
      "    - [A spiking neural model applied to the study of human performance and cognitive decline on Raven's Advanced Progressive Matrices](http://www.sciencedirect.com/science/article/pii/S0160289613001542)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Remembering a Plan\n",
      "\n",
      "- Previously, we talked about a model of the Tower of Hanoi task\n",
      "- Move disks around from one configuration to another\n",
      "- A common situation:\n",
      "    - Trying to move disk 3 to peg A, but disk 2 is in the way, so we need to move disk 2 to peg B, but disk 1 is in the way, so we move disk 1 to peg C\n",
      "    - When we do that, it'd be nice to be able to remember the previous steps in the chain of reasoning\n",
      "    - So we could go back to trying to move disk 2 to peg B, rather than going all the way back to the beginning\n",
      "    - Timing data indicates people do this\n",
      "- What do we need to store?    \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Need to remember disks and what peg they go to\n",
      "    - can't do `D3 + A  + D2 + B`\n",
      "    - since that's the same as `D3 + B  + D2 + A`\n",
      "- Something like: `D3` $\\circledast$ `A + D2` $\\circledast$ `B` \n",
      "- Associative Memory"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Let's start by just computing the convolution"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 64\n",
      "subdim = 8\n",
      "N = 500\n",
      "\n",
      "import nef\n",
      "net=nef.Network('Symbols', fixed_seed=1, quick=True) #Create the network object\n",
      "\n",
      "net.make('A',1,D,mode='direct')\n",
      "net.make('B',1,D,mode='direct')\n",
      "net.make_array('C',N,D/subdim,dimensions=subdim,radius=1.0/math.sqrt(D), seed=2) \n",
      "\n",
      "conv = nef.convolution.make_convolution(net,'*','A','B','C',200, seed=3)\n",
      "\n",
      "net.add_to_nengo()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Now let's extract out a desired answer"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 64\n",
      "subdim = 8\n",
      "N = 500\n",
      "\n",
      "import nef\n",
      "net=nef.Network('Symbols', fixed_seed=1, quick=True) #Create the network object\n",
      "\n",
      "net.make('A',1,D,mode='direct')\n",
      "net.make('B',1,D,mode='direct')\n",
      "net.make_array('C',N,D/subdim,dimensions=subdim,radius=1.0/math.sqrt(D), seed=2) \n",
      "\n",
      "conv = nef.convolution.make_convolution(net,'*','A','B','C',200, seed=3)\n",
      "\n",
      "\n",
      "net.make('E',1,D,mode='direct')\n",
      "net.make('F',1,D,mode='direct')\n",
      "\n",
      "conv = nef.convolution.make_convolution(net,'/','C','E','F',200, invert_second=True, seed=3)\n",
      "\n",
      "\n",
      "net.add_to_nengo()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- But that's not much good without a memory\n",
      "- How do we add one?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "D = 64\n",
      "subdim = 8\n",
      "N = 500\n",
      "\n",
      "import nef\n",
      "net=nef.Network('Symbols', fixed_seed=1, quick=True) #Create the network object\n",
      "\n",
      "net.make('A',1,D,mode='direct')\n",
      "net.make('B',1,D,mode='direct')\n",
      "net.make_array('C',N,D/subdim,dimensions=subdim,radius=1.0/math.sqrt(D), seed=2) \n",
      "\n",
      "net.connect('C', 'C', pstc=0.1)\n",
      "\n",
      "conv = nef.convolution.make_convolution(net,'*','A','B','C',200, seed=3)\n",
      "\n",
      "\n",
      "net.make('E',1,D,mode='direct')\n",
      "net.make('F',1,D,mode='direct')\n",
      "\n",
      "conv = nef.convolution.make_convolution(net,'/','C','E','F',200, invert_second=True, seed=3)\n",
      "\n",
      "\n",
      "net.add_to_nengo()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- Things to note\n",
      "    - Memory slowly decays\n",
      "    - If you push in a new pair for too long, it can wipe out the old pair(s)\n",
      "        - Note that this relies on the saturation behaviour of NEF networks\n",
      "        - Kind of like implicit normalization\n",
      "    - Memory capacity increases with dimensionality\n",
      "        - Also dependent on the number of different possible items in memory (vocabulary size)\n",
      "    - 512 dimensions is suffienct to store ~8 pairs, with a vocabulary size of 100,000 terms\n",
      "        - Note that this is what's needed for storing simple sentences"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}