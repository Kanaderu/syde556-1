{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# SYDE 556/750: Simulating Neurobiological Systems\n",
      "\n",
      "Terry Stewart\n",
      "\n",
      "## Spatial Representation\n",
      "\n",
      "- A common feature that researchers find in the activity of real neurons is that there's some sort of spatial structure in how the neurons are organized\n",
      "    - Topographic maps\n",
      "- Example: retinotopic maps (the layout of neurons maps onto some spatial organization from the retina):\n",
      "    \n",
      "<img src=\"files/lecture9/v1-retinotopy.gif\">\n",
      "\n",
      "- Example: auditory cortex organized by frequency sensitivity\n",
      "\n",
      "<img src=\"files/lecture9/auditory.jpg\">\n",
      "\n",
      "- Example: superior colliculus for eye movement\n",
      "\n",
      "<img src=\"files/lecture9/superior_colliculus.jpg\">\n",
      "\n",
      "- How do we fit this into the NEF?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Spatial structure and encoders\n",
      "\n",
      "- One aspect is nice and easy\n",
      "- Instead of randomly choosing encoders, organize them in a nice way\n",
      "    - the location of the neuron relates to its encoder\n",
      "- Nothing else changes\n",
      "- There's even an option in the interactive mode interface to automatically do this (even while a model is running)\n",
      "- Does this solution handle everything?\n",
      "    - What's missing?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Consider vision\n",
      "\n",
      "- Consider the visual example\n",
      "    - The simplest approach would be to say each neuron has some preferred point in space it responds to\n",
      "    - 2-dimensional representation $x = <h, v>$\n",
      "    - each neuron gets an encoder $e$ based on its position\n",
      "    - $a = G[\\alpha e \\cdot x + J^{bias}]$\n",
      "- That's not quite going to work\n",
      "    - a point $x$ farther away from the origin, but in the same direction as $e$ will make this neuron fire faster\n",
      "    - need to do a change of variables and add an extra dimension\n",
      "    - $x = <h, v, \\sqrt{1 - h^2 -v^2}>$\n",
      "- Now I can give a point as input and the neurons in the correct area of the visual cortex will fire\n",
      "- What's missing here?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Complex representations\n",
      "\n",
      "- But when you see things, you get more than one input at a time\n",
      "- What happens if I see a dot at $<h_0, v_0>$ and another dot at $<h_1, v_1>$ at the same time?\n",
      "- Is that the same as seeing one dot at $<h_0 + h_1, v_0 + v_1>$?\n",
      "    - No\n",
      "- We see them as two distinct points\n",
      "- Same thing with auditory (multiple frequencies at once, plus other sound features)\n",
      "- Maybe even the same thing with the superior colliculus and eye movements (although that's less clear)\n",
      "\n",
      "- So what should this representation be?\n",
      "    - Not just 2-dimensional\n",
      "    - Need to represent a whole image"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Grid-based representation\n",
      "\n",
      "- We know how to represent vectors, so why not just break the visual area up into pixels, and have one dimension per pixel?\n",
      "    - So for a 3x3 image of a square, the $x$ value might be $<1,1,1,1,0,1,1,1,1>$\n",
      "    - Can do brightness (and maybe even colour) this way too\n",
      "- Now choose encoders based on location\n",
      "- Simplest approach: each encoder looks like $<0,0,0,1,0,0,0,0,0>$\n",
      "    - This is equivalent to nine separate ensembles\n",
      "    - What are the limitations of this approach?\n",
      "    - How can we improve this?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Changing how we choose encoders\n",
      "\n",
      "- Perfectly aligned encoders: $<0,0,0,1,0,0,0,0,0>$\n",
      "- Randomly chosen encoders: $<0.3, -0.1, 0.02, 0.1, 0.7, -0.2, -0.3, 0.001, -0.2>$\n",
      "- Can also go inbetween: $<0.25, 0, 0, 0.5, 0.25, 0, 0.25, 0, 0>$\n",
      "- What is that?\n",
      "    - A neuron that responds mostly to dots in one location, but also a little bit to dots nearby\n",
      "- Why would we do this?\n",
      "    - What computational advantage do we get?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Computation with sparse local encoders\n",
      "- With perfectly aligned encoders, we can only do linear functions of $x$\n",
      "- With randomly chosen encoders, we can do any function of $x$, but the accuracy decreases very quickly as we get up to higher-order terms (since there are many pairs of variables)\n",
      "- With encoders that are local (i.e. all the non-zero terms are clustered together) and sparse (i.e. fairly few non-zero terms), we can compute fairly complex function, as long as they are local (i.e. they're dependent on things that are spatially nearby)\n",
      "    - So we'll be worse at things that require us to relate two points far away in space, but better at things where the points are nearby\n",
      "    - This seems like a reasonable tradeoff"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Does the brain do this?\n",
      "\n",
      "- Looks to be the case\n",
      "- Very common way to look at neurons in the visual field\n",
      "- Find the stimulus a neuron most responds to\n",
      "\n",
      "<img src=\"files/lecture9/visual-response.png\">\n",
      "\n",
      "- Usually modelled as a gabor filter (a 2-D sine wave times a Gaussian):\n",
      "\n",
      "<img src=\"files/lecture9/gabors.png\">\n",
      "\n",
      "<img src=\"files/lecture9/gabor-graph.png\">\n",
      "\n",
      "- Pretty much any vision machine learning algorithm gives things like this\n",
      "\n",
      "<img src=\"files/lecture9/ml-visual-curves.png\">\n",
      "\n",
      "- So this could be a good way to generate encoders for visual stimuli\n",
      "    - And maybe even other stimuli too\n",
      "    \n",
      "    \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Pixel size\n",
      "\n",
      "- What size of pixel should we use?\n",
      "- How does that affect things?\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}